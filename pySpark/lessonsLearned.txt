https://www.youtube.com/watch?v=zC9cnh8rJd0

The real reason why spark is usefull is because the integrateion of 
sql, ml, big data (batch and continuous prcessing).
forget hadoop
resilient distributed dataset (RDD)
for any data that we want to process in spark we create a rdd

immutable: (functional)
changes create a new object reference (old doesn't change)
lazy:
compute does not happen until output is requested
https://www.youtube.com/watch?v=XrpSRCwISdk
df          df.show()
df.head(10) df.show(10)
df.columns  {same}
df.dtypes   {same}
df.columns = ['a','b'] df.toDF('a','b','c')# because spark is immutable
df.rename(columns = {'old':'new'}) df.withColumnRenamed('old','new') #again using a tupple
df.drop('mpg',axis=1) df.drop('mpg')
filtering
df[df.mpg <20] {same}
df[(df.mpg <20) & (df.cyl ==6)] {same}
add a column
df['gpm'] = 1/df.mpg df.withColumn('gpm',1/df.mpg)
note div/0 in python = infinity while in pyspark = null
df.fillna(0)<- more options tho {same}
agg
df.groupby(['cyl','gear'])\.agg({'mpg':'mean','disp':'min'}) {same}
don't use numpy for a std tranform in pyspark. Instead use the build in functions because
there is no python execution which keeps it fast

#pd
import numpy as numpy
df['logdisp'] = np.log(df.disp)

#pysp
import pyspark.sql.functions as F
df.withColumn('logdisp',F.log(df.disp))

that being said use python when required
merge/join
left.merge(right,on='key') left.join(right,on='key')
left.merge(right,left_on='a',right_on='b' ) left.join(right,on='key')
pivot table
pd.pivot_table(df,values='D',index=['A','B'], columns=['C'],aggfunc=np.sum) 
df.groupBy('A','B').pivot('c').sum('D')
histogram
df.hist() df.sample(False,0.1).toPandas().hist() # because pyspark df can be insanely huge you will have to select a portiom
# also you will have access to all of the pandas native functions but do not want to convert back
SQL
{null} df.createOrReplaceTempView('foo')
df2 = spark.sql('select * from foo')
pysparks allows you to switch back and forth between sql and df

good practices
- use built-in functions
- use the same version of python and package on cluster as driver (crazy prob warning)
- use SSH port forwarding (learn about it)
- or jupyter hub ()
- sparkMLlib (equivalent to scikit learn)

getting the data from the rdd
https://spark.apache.org/docs/latest/rdd-programming-guide.html
executor
e.g. 10gb -> 9gb (10%saved for stability) -(60% used/(40%s))> 54% total so 5.4 gb

One of the most important capabilities in Spark is persisting (or caching) a dataset in memory across operations. When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). This allows future actions to be much faster (often by more than 10x). Caching is a key tool for iterative algorithms and fast interactive use.
using persist() ro cache() (you can also define the storage level e.g. ram vs disk, etc.)
also once the rdd is created you can't change it. Only create another one
using .coalesce(numOfPartition) is super usefull as it reduces the number of partitions which makes the rdd run at full capacity and hence more efficiently

RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset.

Shared Variables

Normally, when a function passed to a Spark operation (such as map or reduce) is executed on a remote cluster node, it works on separate copies of all the variables used in the function. These variables are copied to each machine, and no updates to the variables on the remote machine are propagated back to the driver program. Supporting general, read-write shared variables across tasks would be inefficient. However, Spark does provide two limited types of shared variables for two common usage patterns: broadcast variables and accumulators.
makes sense why pyspark would use a functional paradigm because you wouldn't want to overwrite any of the data.

unless you call something like an action spark will not do anything.
You will need .collect() 
-----
Data center infrastructure
Data centre tiers are an efficient way to describe the infrastructure components being utilised at a business's data centre. Although a Tier 4 data centre is more complex than a Tier 1 data centre, this does not necessarily mean it is best-suited for a business's needs. While investing in Tier 1 infrastructure might leave a business open to risk, Tier 4 infrastructure might be an over-investment.

    Tier 1: A Tier 1 data centre has a single path for power and cooling and few, if any, redundant and backup components. It has an expected uptime of 99.671% (28.8 hours of downtime annually).
    Tier 2: A Tier 2 data centre has a single path for power and cooling and some redundant and backup components. It has an expected uptime of 99.741% (22 hours of downtime annually).
    Tier 3: A Tier 3 data centre has multiple paths for power and cooling and systems in place to update and maintain it without taking it offline. It has an expected uptime of 99.982% (1.6 hours of downtime annually).
    Tier 4: A Tier 4 data centre is built to be completely fault tolerant and has redundancy for every component. It has an expected uptime of 99.995% (26.3 minutes of downtime annually).
Of course the physical security is top-notch and multiple steps

AWS----
region = independent geo area
availability zone (AZ)= multiple isolate location/data centers within a region
as an aws user you can use any region to deploy your app. s3 buckets works at the region level
different aws services works at different level e.g. s3 region vice ec2 at az level
Amazon Simple Storage Service (s3) simple web services interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. It gives any developer access to the same highly scalable, reliable, fast, inexpensive data storage infrastructure that Amazon uses to run its own global network of web sites.
An Elastic Compute Cloud (EC2) instance is a virtual server that you can use to run applications in Amazon Web Services (AWS). When setting up an EC2 instance, you can custom-configure CPU, storage, memory, and networking resources
scaling
Horizontal scaling means that you scale by adding more machines into your pool of resources whereas Vertical scaling means that you scale by adding more power (CPU, RAM) to an existing machine.
An easy way to remember this is to think of a machine on a server rack, we add more machines across the horizontal direction and add more resources to a machine in the vertical direction.
https://stackoverflow.com/questions/11707879/difference-between-scaling-horizontally-and-vertically-for-databases
prefered horizontal because vertical scaling often requires a restart and is limited by current hardware. Horizontal scaling requires a load balance who can distribute the load, restart a down node, nad redirect flow when a node is down.
amazon has elastic load balancer (elb) for that

A domain name service DNS hosting service is a service that runs Domain Name System servers. Most, but not all, domain name registrars include DNS hosting service with registration.
VPC (vitual private isolate network)

primary security service (access authorization) done using IAM.
KMS (encryption)

aws glue:
data catlog: the metadata of your data
classifier: data mine the datatype of your data
connection: propretries to connect betwee nthe various data stores
crawler: program that connect to our data 
databases: databases
